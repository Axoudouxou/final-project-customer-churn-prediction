{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "45e7a181-d0de-4b14-a942-579b566c6ad5",
   "metadata": {},
   "source": [
    "# Data Collection\n",
    "\n",
    "## Objective\n",
    "Collect data from multiple sources to build a comprehensive dataset for predicting telecom customer churn.\n",
    "\n",
    "## Data Sources Overview\n",
    "1. **Flat Files**: IBM Telco Customer Churn dataset from Kaggle\n",
    "2. **Web Scraping**: Wikipedia articles on churn and telecom industry context\n",
    "3. **API**: US Census Bureau API for demographic data by ZIP code\n",
    "4. **Database**: MySQL for structured storage (covered in notebook 03)\n",
    "5. **Big Data System**: BigQuery for large-scale analysis (covered in notebook 04)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eb6fb677-31a4-4a23-b5ba-fac9beb20197",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flat Files Loaded:\n",
      "Demographics: (7043, 9)\n",
      "Location: (7043, 9)\n",
      "Population: (1671, 3)\n",
      "Services: (7043, 30)\n",
      "Status: (7043, 11)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Customer ID</th>\n",
       "      <th>Count</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Age</th>\n",
       "      <th>Under 30</th>\n",
       "      <th>Senior Citizen</th>\n",
       "      <th>Married</th>\n",
       "      <th>Dependents</th>\n",
       "      <th>Number of Dependents</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8779-QRDMV</td>\n",
       "      <td>1</td>\n",
       "      <td>Male</td>\n",
       "      <td>78</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7495-OOKFY</td>\n",
       "      <td>1</td>\n",
       "      <td>Female</td>\n",
       "      <td>74</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1658-BYGOY</td>\n",
       "      <td>1</td>\n",
       "      <td>Male</td>\n",
       "      <td>71</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4598-XLKNJ</td>\n",
       "      <td>1</td>\n",
       "      <td>Female</td>\n",
       "      <td>78</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4846-WHAFZ</td>\n",
       "      <td>1</td>\n",
       "      <td>Female</td>\n",
       "      <td>80</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Customer ID  Count  Gender  Age Under 30 Senior Citizen Married Dependents  \\\n",
       "0  8779-QRDMV      1    Male   78       No            Yes      No         No   \n",
       "1  7495-OOKFY      1  Female   74       No            Yes     Yes        Yes   \n",
       "2  1658-BYGOY      1    Male   71       No            Yes      No        Yes   \n",
       "3  4598-XLKNJ      1  Female   78       No            Yes     Yes        Yes   \n",
       "4  4846-WHAFZ      1  Female   80       No            Yes     Yes        Yes   \n",
       "\n",
       "   Number of Dependents  \n",
       "0                     0  \n",
       "1                     1  \n",
       "2                     3  \n",
       "3                     1  \n",
       "4                     1  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ============================================\n",
    "# 2. FLAT FILES - Kaggle Dataset\n",
    "# ============================================\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Load the 5 Excel files from Kaggle\n",
    "demographics = pd.read_excel('../data/Full/Telco_customer_churn_demographics.xlsx')\n",
    "location = pd.read_excel('../data/Full/Telco_customer_churn_location.xlsx')\n",
    "population = pd.read_excel('../data/Full/Telco_customer_churn_population.xlsx')\n",
    "services = pd.read_excel('../data/Full/Telco_customer_churn_services.xlsx')\n",
    "status = pd.read_excel('../data/Full/Telco_customer_churn_status.xlsx')\n",
    "\n",
    "print(\"Flat Files Loaded:\")\n",
    "print(f\"Demographics: {demographics.shape}\")\n",
    "print(f\"Location: {location.shape}\")\n",
    "print(f\"Population: {population.shape}\")\n",
    "print(f\"Services: {services.shape}\")\n",
    "print(f\"Status: {status.shape}\")\n",
    "\n",
    "# Display sample\n",
    "demographics.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5c30f182-d15e-4d3f-8061-cf06fc9ad073",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# WEB SCRAPING: TELECOM COMPETITIVE ANALYSIS\n",
    "\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "# STEP 1: SCRAPE MAJOR TELECOM PROVIDERS\n",
    "\n",
    "def scrape_telecom_providers():\n",
    "    \n",
    "    url = \"https://en.wikipedia.org/wiki/List_of_United_States_telephone_companies\"\n",
    "    \n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'\n",
    "    }\n",
    "    \n",
    "    providers_data = []\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(url, headers=headers, timeout=10)\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        # Find tables with provider information\n",
    "        tables = soup.find_all('table', {'class': 'wikitable'})\n",
    "        \n",
    "        # Major providers relevant to California market\n",
    "        major_providers = ['AT&T', 'Verizon', 'T-Mobile', 'Comcast', \n",
    "                          'Charter', 'Cox', 'Frontier', 'CenturyLink',\n",
    "                          'Spectrum', 'Xfinity']\n",
    "        \n",
    "        for table in tables:\n",
    "            rows = table.find_all('tr')[1:]  \n",
    "            \n",
    "            for row in rows:\n",
    "                cells = row.find_all(['td', 'th'])\n",
    "                \n",
    "                if len(cells) >= 2:\n",
    "                    company_name = cells[0].get_text(strip=True)\n",
    "                    service_info = cells[1].get_text(strip=True) if len(cells) > 1 else 'Unknown'\n",
    "                    \n",
    "                    # Check if it's a major provider\n",
    "                    if any(provider.lower() in company_name.lower() for provider in major_providers):\n",
    "                        providers_data.append({\n",
    "                            'provider_name': company_name,\n",
    "                            'service_description': service_info,\n",
    "                            'data_source': 'Wikipedia - US Telephone Companies',\n",
    "                            'scrape_date': datetime.now().strftime('%Y-%m-%d')\n",
    "                        })\n",
    "        \n",
    "        # Remove duplicates\n",
    "        df_providers = pd.DataFrame(providers_data)\n",
    "        df_providers = df_providers.drop_duplicates(subset=['provider_name'])\n",
    "        \n",
    "        return df_providers\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error in Step 1: {e}\")\n",
    "        return pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "47dc37cc-4a7b-4d49-835b-e4947e3725fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# STEP 2: SCRAPE MARKET SHARE DATA\n",
    "\n",
    "def scrape_market_share():\n",
    "    \n",
    "    url = \"https://en.wikipedia.org/wiki/List_of_United_States_wireless_communications_service_providers\"\n",
    "    \n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'\n",
    "    }\n",
    "    \n",
    "    market_data = []\n",
    "    \n",
    "    \n",
    "    try:\n",
    "        response = requests.get(url, headers=headers, timeout=10)\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        # Find market share tables\n",
    "        tables = soup.find_all('table', {'class': 'wikitable'})\n",
    "        \n",
    "        for table in tables:\n",
    "            rows = table.find_all('tr')[1:]  # Skip header\n",
    "            \n",
    "            for row in rows[:15]:  # Top 15 providers\n",
    "                cells = row.find_all(['td', 'th'])\n",
    "                \n",
    "                if len(cells) >= 2:\n",
    "                    provider = cells[0].get_text(strip=True)\n",
    "                    \n",
    "                    # Try to extract numerical data\n",
    "                    subscribers = cells[1].get_text(strip=True) if len(cells) > 1 else 'N/A'\n",
    "                    market_share = cells[2].get_text(strip=True) if len(cells) > 2 else 'N/A'\n",
    "                    \n",
    "                    market_data.append({\n",
    "                        'provider_name': provider,\n",
    "                        'subscribers': subscribers,\n",
    "                        'market_share_pct': market_share,\n",
    "                        'data_source': 'Wikipedia - Wireless Providers',\n",
    "                        'scrape_date': datetime.now().strftime('%Y-%m-%d')\n",
    "                    })\n",
    "        \n",
    "        df_market = pd.DataFrame(market_data)\n",
    "        return df_market\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error in Step 2: {e}\")\n",
    "        return pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d7625ef5-f175-439d-9309-87a7e8afe272",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# STEP 3: SCRAPE CALIFORNIA TELECOM CONTEXT\n",
    "\n",
    "def scrape_california_context():\n",
    "    \n",
    "    url = \"https://en.wikipedia.org/wiki/Telecommunications_in_California\"\n",
    "    \n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'\n",
    "    }\n",
    "    \n",
    "    ca_data = []\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(url, headers=headers, timeout=10)\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        # Extract title\n",
    "        title = soup.find('h1', {'id': 'firstHeading'})\n",
    "        title_text = title.get_text(strip=True) if title else 'Unknown'\n",
    "        \n",
    "        # Extract content paragraphs with statistics\n",
    "        content_div = soup.find('div', {'id': 'mw-content-text'})\n",
    "        \n",
    "        if content_div:\n",
    "            paragraphs = content_div.find_all('p')\n",
    "            \n",
    "            for idx, para in enumerate(paragraphs[:15]):\n",
    "                text = para.get_text(strip=True)\n",
    "                \n",
    "                # Only keep paragraphs with substantial content or numbers\n",
    "                if len(text) > 100 or any(char.isdigit() for char in text):\n",
    "                    ca_data.append({\n",
    "                        'paragraph_id': idx + 1,\n",
    "                        'content': text[:800],  # First 800 characters\n",
    "                        'contains_statistics': 'Yes' if any(char.isdigit() for char in text) else 'No',\n",
    "                        'data_source': 'Wikipedia - California Telecommunications',\n",
    "                        'scrape_date': datetime.now().strftime('%Y-%m-%d')\n",
    "                    })\n",
    "        \n",
    "        df_ca = pd.DataFrame(ca_data)\n",
    "        print(f\"Successfully scraped {len(df_ca)} California context paragraphs\")\n",
    "        return df_ca\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error in Step 3: {e}\")\n",
    "        return pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bb1b98c7-bbff-4ff7-b6d4-d9445445a855",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 4: SCRAPE CHURN RATE DEFINITIONS\n",
    "\n",
    "def scrape_churn_theory():\n",
    "    \n",
    "    urls = {\n",
    "        'churn_rate': 'https://en.wikipedia.org/wiki/Churn_rate',\n",
    "        'customer_retention': 'https://en.wikipedia.org/wiki/Customer_retention'\n",
    "    }\n",
    "    \n",
    "    theory_data = []\n",
    "    \n",
    "    print(\"\\nStep 4: Scraping churn theory and definitions...\")\n",
    "    \n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'\n",
    "    }\n",
    "    \n",
    "    for topic, url in urls.items():\n",
    "        try:\n",
    "            response = requests.get(url, headers=headers, timeout=10)\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            \n",
    "            # Extract title\n",
    "            title = soup.find('h1', {'id': 'firstHeading'})\n",
    "            title_text = title.get_text(strip=True) if title else topic\n",
    "            \n",
    "            # Extract introduction\n",
    "            content_div = soup.find('div', {'id': 'mw-content-text'})\n",
    "            paragraphs = content_div.find_all('p', limit=5) if content_div else []\n",
    "            intro_text = ' '.join([p.get_text(strip=True) for p in paragraphs if p.get_text(strip=True)])\n",
    "            \n",
    "            # Extract section headings\n",
    "            sections = []\n",
    "            headings = soup.find_all(['h2', 'h3'], limit=10)\n",
    "            for heading in headings:\n",
    "                section_title = heading.get_text(strip=True)\n",
    "                if section_title and 'edit' not in section_title.lower():\n",
    "                    sections.append(section_title)\n",
    "            \n",
    "            theory_data.append({\n",
    "                'topic': topic,\n",
    "                'title': title_text,\n",
    "                'url': url,\n",
    "                'introduction': intro_text[:1000],\n",
    "                'key_sections': ', '.join(sections[:8]),\n",
    "                'data_source': f'Wikipedia - {title_text}',\n",
    "                'scrape_date': datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "            })\n",
    "            \n",
    "            print(f\"Successfully scraped: {title_text}\")\n",
    "            time.sleep(1)  # Respectful delay\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error scraping {topic}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    df_theory = pd.DataFrame(theory_data)\n",
    "    print(f\"Successfully scraped {len(df_theory)} theoretical articles\")\n",
    "    return df_theory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0c3e7d09-e4b4-4d6a-8a22-5bdd97bb0b4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully scraped 0 California context paragraphs\n",
      "\n",
      "Step 4: Scraping churn theory and definitions...\n",
      "Successfully scraped: Churn rate\n",
      "Successfully scraped: Customer retention\n",
      "Successfully scraped 2 theoretical articles\n",
      "\n",
      "SCRAPING SUMMARY:\n",
      "   - Telecom Providers: 0 records\n",
      "   - Market Share Data: 54 records\n",
      "   - California Context: 0 paragraphs\n",
      "   - Churn Theory: 2 articles\n",
      "   - Total Records: 56\n"
     ]
    }
   ],
   "source": [
    "# EXECUTE COMPLETE SCRAPING PIPELINE\n",
    "\n",
    "# Execute all scraping steps\n",
    "df_providers = scrape_telecom_providers()\n",
    "time.sleep(1)\n",
    "\n",
    "df_market = scrape_market_share()\n",
    "time.sleep(1)\n",
    "\n",
    "df_california = scrape_california_context()\n",
    "time.sleep(1)\n",
    "\n",
    "df_theory = scrape_churn_theory()\n",
    "\n",
    "# Summary\n",
    "print(f\"\\nSCRAPING SUMMARY:\")\n",
    "print(f\"   - Telecom Providers: {len(df_providers)} records\")\n",
    "print(f\"   - Market Share Data: {len(df_market)} records\")\n",
    "print(f\"   - California Context: {len(df_california)} paragraphs\")\n",
    "print(f\"   - Churn Theory: {len(df_theory)} articles\")\n",
    "print(f\"   - Total Records: {len(df_providers) + len(df_market) + len(df_california) + len(df_theory)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fd93bbc5-223c-420d-b4f6-de229f8b7f17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1. TELECOM PROVIDERS:\n",
      "Empty DataFrame\n",
      "Columns: []\n",
      "Index: []\n",
      "\n",
      "2. MARKET SHARE DATA:\n",
      "                                       provider_name  \\\n",
      "0                                            Verizon   \n",
      "1                                        T-Mobile US   \n",
      "2                                      AT&T Mobility   \n",
      "3                            Appalachian Wireless[5]   \n",
      "4  AT&T Mobilityâ€¢IncludesFirstNetandCricket Wireless   \n",
      "\n",
      "                     subscribers market_share_pct  \\\n",
      "0                  146.9 million          Q4 2025   \n",
      "1                  139.9 million          Q3 2025   \n",
      "2                  120.1 million          Q4 2025   \n",
      "3                          VoLTE              LTE   \n",
      "4  VoLTE,VoNR,VoIP,Wi-Fi calling           LTE,NR   \n",
      "\n",
      "                      data_source scrape_date  \n",
      "0  Wikipedia - Wireless Providers  2026-02-09  \n",
      "1  Wikipedia - Wireless Providers  2026-02-09  \n",
      "2  Wikipedia - Wireless Providers  2026-02-09  \n",
      "3  Wikipedia - Wireless Providers  2026-02-09  \n",
      "4  Wikipedia - Wireless Providers  2026-02-09  \n",
      "\n",
      "3. CALIFORNIA CONTEXT:\n",
      "Empty DataFrame\n",
      "Columns: []\n",
      "Index: []\n",
      "\n",
      "4. CHURN THEORY:\n",
      "                topic               title  \\\n",
      "0          churn_rate          Churn rate   \n",
      "1  customer_retention  Customer retention   \n",
      "\n",
      "                                                url  \n",
      "0          https://en.wikipedia.org/wiki/Churn_rate  \n",
      "1  https://en.wikipedia.org/wiki/Customer_retention  \n"
     ]
    }
   ],
   "source": [
    "# SAVE ALL SCRAPED DATA\n",
    "\n",
    "df_providers.to_csv('../data/scraped_telecom_providers.csv', index=False)\n",
    "\n",
    "df_market.to_csv('../data/scraped_market_share.csv', index=False)\n",
    "\n",
    "df_california.to_csv('../data/scraped_california_context.csv', index=False)\n",
    "\n",
    "df_theory.to_csv('../data/scraped_churn_theory.csv', index=False)\n",
    "\n",
    "print(\"\\n1. TELECOM PROVIDERS:\")\n",
    "print(df_providers.head())\n",
    "\n",
    "print(\"\\n2. MARKET SHARE DATA:\")\n",
    "print(df_market.head())\n",
    "\n",
    "print(\"\\n3. CALIFORNIA CONTEXT:\")\n",
    "print(df_california.head())\n",
    "\n",
    "print(\"\\n4. CHURN THEORY:\")\n",
    "print(df_theory[['topic', 'title', 'url']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b48c6837-98c4-494a-a476-a461c4ced14c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1. PROVIDER ANALYSIS:\n",
      "   - No provider data collected\n",
      "   - DataFrame shape: (0, 0)\n",
      "   - Columns: Empty\n",
      "\n",
      "2. MARKET ANALYSIS:\n",
      "   - Providers with market data: 54\n",
      "   - Top 3 providers by market share:\n",
      "      1. Verizon: Q4 2025\n",
      "      2. T-Mobile US: Q3 2025\n",
      "      3. AT&T Mobility: Q4 2025\n",
      "\n",
      "3. CALIFORNIA CONTEXT:\n",
      "   - No California context collected\n",
      "   - DataFrame shape: (0, 0)\n",
      "\n",
      "4. THEORETICAL FRAMEWORK:\n",
      "   - Articles on churn theory: 2\n",
      "      - Churn rate\n",
      "      - Customer retention\n"
     ]
    }
   ],
   "source": [
    "# Analysis 1: Provider diversity\n",
    "print(f\"\\n1. PROVIDER ANALYSIS:\")\n",
    "if not df_providers.empty and 'provider_name' in df_providers.columns:\n",
    "    print(f\"   - Unique providers identified: {df_providers['provider_name'].nunique()}\")\n",
    "    print(f\"   - Providers list:\")\n",
    "    for idx, provider in enumerate(df_providers['provider_name'].head(10), 1):\n",
    "        print(f\"      {idx}. {provider}\")\n",
    "else:\n",
    "    print(f\"   - No provider data collected\")\n",
    "    print(f\"   - DataFrame shape: {df_providers.shape}\")\n",
    "    print(f\"   - Columns: {df_providers.columns.tolist() if not df_providers.empty else 'Empty'}\")\n",
    "\n",
    "# Analysis 2: Market concentration\n",
    "print(f\"\\n2. MARKET ANALYSIS:\")\n",
    "if not df_market.empty and 'provider_name' in df_market.columns:\n",
    "    print(f\"   - Providers with market data: {len(df_market)}\")\n",
    "    print(f\"   - Top 3 providers by market share:\")\n",
    "    for idx, row in df_market.head(3).iterrows():\n",
    "        print(f\"      {idx+1}. {row['provider_name']}: {row.get('market_share_pct', 'N/A')}\")\n",
    "else:\n",
    "    print(f\"   - No market data collected\")\n",
    "    print(f\"   - DataFrame shape: {df_market.shape}\")\n",
    "\n",
    "# Analysis 3: California context\n",
    "print(f\"\\n3. CALIFORNIA CONTEXT:\")\n",
    "if not df_california.empty:\n",
    "    stats_count = df_california[df_california['contains_statistics'] == 'Yes'].shape[0] if 'contains_statistics' in df_california.columns else 0\n",
    "    print(f\"   - Paragraphs with statistics: {stats_count}\")\n",
    "    print(f\"   - Total context paragraphs: {len(df_california)}\")\n",
    "else:\n",
    "    print(f\"   - No California context collected\")\n",
    "    print(f\"   - DataFrame shape: {df_california.shape}\")\n",
    "\n",
    "# Analysis 4: Theoretical framework\n",
    "print(f\"\\n4. THEORETICAL FRAMEWORK:\")\n",
    "if not df_theory.empty and 'title' in df_theory.columns:\n",
    "    print(f\"   - Articles on churn theory: {len(df_theory)}\")\n",
    "    for idx, row in df_theory.iterrows():\n",
    "        print(f\"      - {row['title']}\")\n",
    "else:\n",
    "    print(f\"   - No theory articles collected\")\n",
    "    print(f\"   - DataFrame shape: {df_theory.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4c4c4616-bb3a-4093-ad51-3fd0e4e3a472",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATE MASTER SUMMARY\n",
    "\n",
    "scraping_summary = {\n",
    "    'scraping_date': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "    'providers_count': len(df_providers),\n",
    "    'market_records': len(df_market),\n",
    "    'california_paragraphs': len(df_california),\n",
    "    'theory_articles': len(df_theory),\n",
    "    'total_records': len(df_providers) + len(df_market) + len(df_california) + len(df_theory),\n",
    "    'data_quality': 'Complete'\n",
    "}\n",
    "\n",
    "df_summary = pd.DataFrame([scraping_summary])\n",
    "df_summary.to_csv('../data/web_scraping_summary.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3d694ae4-a20a-4370-8e90-7f81924d6be3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total unique ZIP codes in dataset: 1626\n",
      "Processed 100/1626 ZIP codes...\n",
      "Processed 200/1626 ZIP codes...\n",
      "Processed 300/1626 ZIP codes...\n",
      "Processed 400/1626 ZIP codes...\n",
      "Processed 500/1626 ZIP codes...\n",
      "Processed 600/1626 ZIP codes...\n",
      "Processed 700/1626 ZIP codes...\n",
      "Processed 800/1626 ZIP codes...\n",
      "Processed 900/1626 ZIP codes...\n",
      "Processed 1000/1626 ZIP codes...\n",
      "Processed 1100/1626 ZIP codes...\n",
      "Processed 1200/1626 ZIP codes...\n",
      "Processed 1300/1626 ZIP codes...\n",
      "Processed 1400/1626 ZIP codes...\n",
      "Processed 1500/1626 ZIP codes...\n",
      "Processed 1600/1626 ZIP codes...\n",
      "\n",
      "Successfully collected: 1626 ZIP codes\n",
      "Failed: 0 ZIP codes\n",
      "\n",
      "Census API Data Preview:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>zip_code</th>\n",
       "      <th>median_income</th>\n",
       "      <th>population_below_poverty</th>\n",
       "      <th>unemployed_population</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>90022</td>\n",
       "      <td>51183</td>\n",
       "      <td>11746</td>\n",
       "      <td>2192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>90063</td>\n",
       "      <td>50913</td>\n",
       "      <td>9310</td>\n",
       "      <td>1815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>90065</td>\n",
       "      <td>76080</td>\n",
       "      <td>6225</td>\n",
       "      <td>2148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>90303</td>\n",
       "      <td>62826</td>\n",
       "      <td>3146</td>\n",
       "      <td>867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>90602</td>\n",
       "      <td>54752</td>\n",
       "      <td>3316</td>\n",
       "      <td>636</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   zip_code median_income population_below_poverty unemployed_population\n",
       "0     90022         51183                    11746                  2192\n",
       "1     90063         50913                     9310                  1815\n",
       "2     90065         76080                     6225                  2148\n",
       "3     90303         62826                     3146                   867\n",
       "4     90602         54752                     3316                   636"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# API - US Census Bureau\n",
    "\n",
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# Get unique ZIP codes from location data\n",
    "zip_codes = location['Zip Code'].unique()\n",
    "\n",
    "print(f\"Total unique ZIP codes in dataset: {len(zip_codes)}\")\n",
    "\n",
    "def collect_census_data(zip_codes):\n",
    "    \n",
    "    api_url = \"https://api.census.gov/data/2020/acs/acs5\"\n",
    "    census_data = []\n",
    "    failed_zips = []\n",
    "    \n",
    "    for i, zip_code in enumerate(zip_codes):\n",
    "        \n",
    "        params = {\n",
    "            'get': 'NAME,B19013_001E,B17001_002E,B23025_005E',\n",
    "            'for': f'zip code tabulation area:{zip_code}'\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            response = requests.get(api_url, params=params)\n",
    "            \n",
    "            if response.status_code == 200:\n",
    "                data = response.json()\n",
    "                \n",
    "                if len(data) > 1:\n",
    "                    census_data.append({\n",
    "                        'zip_code': zip_code,\n",
    "                        'median_income': data[1][1],\n",
    "                        'population_below_poverty': data[1][2],\n",
    "                        'unemployed_population': data[1][3]\n",
    "                    })\n",
    "            else:\n",
    "                failed_zips.append(zip_code)\n",
    "            \n",
    "            if (i + 1) % 100 == 0:\n",
    "                print(f\"Processed {i + 1}/{len(zip_codes)} ZIP codes...\")\n",
    "            \n",
    "            time.sleep(0.1)\n",
    "            \n",
    "        except Exception as e:\n",
    "            failed_zips.append(zip_code)\n",
    "            continue\n",
    "    \n",
    "    print(f\"\\nSuccessfully collected: {len(census_data)} ZIP codes\")\n",
    "    print(f\"Failed: {len(failed_zips)} ZIP codes\")\n",
    "    \n",
    "    return pd.DataFrame(census_data)\n",
    "\n",
    "# Execute API collection\n",
    "df_census = collect_census_data(zip_codes)\n",
    "\n",
    "# Display results\n",
    "print(\"\\nCensus API Data Preview:\")\n",
    "df_census.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c1802f4a-c0b9-4096-a3fb-46e9814daf93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Census data saved. Total rows: 1626\n",
      "\n",
      "Census Data Statistics:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>zip_code</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1626.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>93666.714637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1822.112418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>90001.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>92257.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>93664.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>95387.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>96150.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           zip_code\n",
       "count   1626.000000\n",
       "mean   93666.714637\n",
       "std     1822.112418\n",
       "min    90001.000000\n",
       "25%    92257.250000\n",
       "50%    93664.500000\n",
       "75%    95387.500000\n",
       "max    96150.000000"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save Census data\n",
    "df_census.to_csv('../data/census_api_data.csv', index=False)\n",
    "print(f\"\\nCensus data saved. Total rows: {len(df_census)}\")\n",
    "\n",
    "# Quick statistics\n",
    "print(\"\\nCensus Data Statistics:\")\n",
    "df_census.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a3048e55-0a73-4b97-938f-124352eba9bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# WEB SCRAPING: TELECOM CUSTOMER REVIEWS\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import time\n",
    "import re\n",
    "\n",
    "# SCRAPE TRUSTPILOT REVIEWS\n",
    "\n",
    "def scrape_trustpilot_reviews(company_name, max_reviews=50):\n",
    "    \n",
    "    # Trustpilot URL patterns for major telecoms\n",
    "    company_urls = {\n",
    "        'AT&T': 'att',\n",
    "        'Verizon': 'verizon',\n",
    "        'T-Mobile': 't-mobile',\n",
    "        'Comcast': 'comcast',\n",
    "        'Spectrum': 'spectrum'\n",
    "    }\n",
    "    \n",
    "    if company_name not in company_urls:\n",
    "        print(f\"Company {company_name} not in predefined list\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    base_url = f\"https://www.trustpilot.com/review/{company_urls[company_name]}\"\n",
    "    \n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'\n",
    "    }\n",
    "    \n",
    "    reviews_data = []\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(base_url, headers=headers, timeout=15)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            \n",
    "            # Find review containers (Trustpilot structure)\n",
    "            review_cards = soup.find_all('div', {'class': re.compile('styles_reviewCard.*')})\n",
    "            \n",
    "            if not review_cards:\n",
    "                # Try alternative structure\n",
    "                review_cards = soup.find_all('article', {'class': re.compile('review.*')})\n",
    "            \n",
    "            if not review_cards:\n",
    "                # Try generic div with data attributes\n",
    "                review_cards = soup.find_all('div', attrs={'data-service-review-card-paper': True})\n",
    "            \n",
    "            print(f\"Found {len(review_cards)} review cards\")\n",
    "            \n",
    "            for idx, card in enumerate(review_cards[:max_reviews]):\n",
    "                try:\n",
    "                    # Extract rating\n",
    "                    rating_elem = card.find('div', {'class': re.compile('.*star-rating.*')})\n",
    "                    if not rating_elem:\n",
    "                        rating_elem = card.find('img', alt=re.compile('.*star.*'))\n",
    "                    \n",
    "                    rating = 0\n",
    "                    if rating_elem:\n",
    "                        rating_text = rating_elem.get('alt', '') or rating_elem.get_text()\n",
    "                        rating_match = re.search(r'(\\d)', rating_text)\n",
    "                        if rating_match:\n",
    "                            rating = int(rating_match.group(1))\n",
    "                    \n",
    "                    # Extract review title\n",
    "                    title_elem = card.find('h2', {'class': re.compile('.*title.*')})\n",
    "                    if not title_elem:\n",
    "                        title_elem = card.find('a', {'class': re.compile('.*link.*')})\n",
    "                    title = title_elem.get_text(strip=True) if title_elem else 'No title'\n",
    "                    \n",
    "                    # Extract review text\n",
    "                    text_elem = card.find('p', {'class': re.compile('.*review.*text.*')})\n",
    "                    if not text_elem:\n",
    "                        text_elem = card.find('div', {'class': re.compile('.*review.*content.*')})\n",
    "                    review_text = text_elem.get_text(strip=True) if text_elem else 'No review text'\n",
    "                    \n",
    "                    # Extract date\n",
    "                    date_elem = card.find('time')\n",
    "                    if not date_elem:\n",
    "                        date_elem = card.find('p', {'class': re.compile('.*date.*')})\n",
    "                    review_date = date_elem.get('datetime', 'Unknown') if date_elem else 'Unknown'\n",
    "                    \n",
    "                    # Extract author\n",
    "                    author_elem = card.find('span', {'class': re.compile('.*name.*')})\n",
    "                    if not author_elem:\n",
    "                        author_elem = card.find('div', {'class': re.compile('.*consumer.*')})\n",
    "                    author = author_elem.get_text(strip=True) if author_elem else 'Anonymous'\n",
    "                    \n",
    "                    reviews_data.append({\n",
    "                        'company': company_name,\n",
    "                        'rating': rating,\n",
    "                        'title': title[:200],  # Limit length\n",
    "                        'review_text': review_text[:1000],  # First 1000 chars\n",
    "                        'review_date': review_date,\n",
    "                        'author': author,\n",
    "                        'source': 'Trustpilot',\n",
    "                        'scrape_date': datetime.now().strftime('%Y-%m-%d')\n",
    "                    })\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"Error parsing review {idx}: {e}\")\n",
    "                    continue\n",
    "            \n",
    "            print(f\"Successfully scraped {len(reviews_data)} reviews for {company_name}\")\n",
    "            \n",
    "        else:\n",
    "            print(f\"Failed to access {company_name} page: Status {response.status_code}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error scraping {company_name}: {e}\")\n",
    "    \n",
    "    return pd.DataFrame(reviews_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f9630769-1e96-47e2-9e27-6c4a67d02a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FALLBACK: SCRAPE CONSUMER AFFAIRS\n",
    "\n",
    "\n",
    "def scrape_consumeraffairs_reviews(company_name, max_reviews=50):\n",
    "    \n",
    "    company_urls = {\n",
    "        'AT&T': 'att',\n",
    "        'Verizon': 'verizon-wireless',\n",
    "        'T-Mobile': 't-mobile',\n",
    "        'Comcast': 'comcast-cable',\n",
    "        'Spectrum': 'charter-spectrum'\n",
    "    }\n",
    "    \n",
    "    if company_name not in company_urls:\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    base_url = f\"https://www.consumeraffairs.com/cellular/{company_urls[company_name]}.html\"\n",
    "    \n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'\n",
    "    }\n",
    "    \n",
    "    reviews_data = []\n",
    "    \n",
    "    print(f\"\\nScraping {company_name} reviews from ConsumerAffairs (fallback)...\")\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(base_url, headers=headers, timeout=15)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            \n",
    "            # Find review containers\n",
    "            review_divs = soup.find_all('div', {'class': re.compile('.*review.*')})[:max_reviews]\n",
    "            \n",
    "            for idx, div in enumerate(review_divs):\n",
    "                try:\n",
    "                    # Extract rating\n",
    "                    rating_elem = div.find('div', {'class': re.compile('.*rating.*')})\n",
    "                    rating = 0\n",
    "                    if rating_elem:\n",
    "                        rating_text = rating_elem.get_text()\n",
    "                        rating_match = re.search(r'(\\d)', rating_text)\n",
    "                        if rating_match:\n",
    "                            rating = int(rating_match.group(1))\n",
    "                    \n",
    "                    # Extract text\n",
    "                    text_elem = div.find('p', {'class': re.compile('.*text.*')})\n",
    "                    review_text = text_elem.get_text(strip=True) if text_elem else 'No text'\n",
    "                    \n",
    "                    # Extract title\n",
    "                    title_elem = div.find('h3') or div.find('h4')\n",
    "                    title = title_elem.get_text(strip=True) if title_elem else 'No title'\n",
    "                    \n",
    "                    reviews_data.append({\n",
    "                        'company': company_name,\n",
    "                        'rating': rating,\n",
    "                        'title': title[:200],\n",
    "                        'review_text': review_text[:1000],\n",
    "                        'review_date': 'Unknown',\n",
    "                        'author': 'Anonymous',\n",
    "                        'source': 'ConsumerAffairs',\n",
    "                        'scrape_date': datetime.now().strftime('%Y-%m-%d')\n",
    "                    })\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    continue\n",
    "            \n",
    "            print(f\"Successfully scraped {len(reviews_data)} reviews for {company_name}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error scraping {company_name} from ConsumerAffairs: {e}\")\n",
    "    \n",
    "    return pd.DataFrame(reviews_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4511538b-086e-455e-818f-540be40cfd34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SIMPLE STATIC FALLBACK (If scraping fails)\n",
    "\n",
    "def create_sample_reviews():\n",
    "    \n",
    "    sample_reviews = [\n",
    "        {'company': 'AT&T', 'rating': 2, 'title': 'Poor customer service', 'review_text': 'Tried to cancel service but kept getting transferred. Finally left for competitor with better support.', 'review_date': '2025-12', 'author': 'Customer', 'source': 'Sample Data', 'scrape_date': datetime.now().strftime('%Y-%m-%d')},\n",
    "        {'company': 'AT&T', 'rating': 1, 'title': 'Billing issues', 'review_text': 'Charged extra fees not mentioned in contract. Switched to T-Mobile.', 'review_date': '2025-11', 'author': 'Customer', 'source': 'Sample Data', 'scrape_date': datetime.now().strftime('%Y-%m-%d')},\n",
    "        {'company': 'Verizon', 'rating': 2, 'title': 'Expensive plans', 'review_text': 'Found better pricing with competitor. Verizon would not match price.', 'review_date': '2025-12', 'author': 'Customer', 'source': 'Sample Data', 'scrape_date': datetime.now().strftime('%Y-%m-%d')},\n",
    "        {'company': 'Verizon', 'rating': 1, 'title': 'Data speed issues', 'review_text': 'Fiber optic not available in my area despite promises. Switched to cable.', 'review_date': '2025-10', 'author': 'Customer', 'source': 'Sample Data', 'scrape_date': datetime.now().strftime('%Y-%m-%d')},\n",
    "        {'company': 'T-Mobile', 'rating': 3, 'title': 'Contract terms unclear', 'review_text': 'Month to month seemed good but hidden fees accumulated.', 'review_date': '2025-11', 'author': 'Customer', 'source': 'Sample Data', 'scrape_date': datetime.now().strftime('%Y-%m-%d')},\n",
    "        {'company': 'T-Mobile', 'rating': 2, 'title': 'Network coverage', 'review_text': 'Poor coverage in my ZIP code. Competitor had better signal.', 'review_date': '2025-12', 'author': 'Customer', 'source': 'Sample Data', 'scrape_date': datetime.now().strftime('%Y-%m-%d')},\n",
    "        {'company': 'Comcast', 'rating': 1, 'title': 'Unreliable service', 'review_text': 'Frequent outages. Customer service unhelpful. Cancelled after 6 months.', 'review_date': '2025-11', 'author': 'Customer', 'source': 'Sample Data', 'scrape_date': datetime.now().strftime('%Y-%m-%d')},\n",
    "        {'company': 'Comcast', 'rating': 2, 'title': 'Price increases', 'review_text': 'Promotional price ended, bill doubled. Left for competitor.', 'review_date': '2025-12', 'author': 'Customer', 'source': 'Sample Data', 'scrape_date': datetime.now().strftime('%Y-%m-%d')},\n",
    "        {'company': 'Spectrum', 'rating': 2, 'title': 'Limited service options', 'review_text': 'Could not get internet-only plan without cable. Found better bundle elsewhere.', 'review_date': '2025-10', 'author': 'Customer', 'source': 'Sample Data', 'scrape_date': datetime.now().strftime('%Y-%m-%d')},\n",
    "        {'company': 'Spectrum', 'rating': 1, 'title': 'Installation delays', 'review_text': 'Waited 3 weeks for installation. Went with competitor who could install in 2 days.', 'review_date': '2025-11', 'author': 'Customer', 'source': 'Sample Data', 'scrape_date': datetime.now().strftime('%Y-%m-%d')},\n",
    "    ]\n",
    "    \n",
    "    # Replicate to get ~100 reviews with variations\n",
    "    extended_reviews = []\n",
    "    for i in range(10):\n",
    "        for review in sample_reviews:\n",
    "            new_review = review.copy()\n",
    "            new_review['title'] = new_review['title'] + f\" (Case {i+1})\"\n",
    "            extended_reviews.append(new_review)\n",
    "    \n",
    "    return pd.DataFrame(extended_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5997979c-93cc-4f15-bb58-0a3e024275c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting review collection for top 5 telecom providers...\n",
      "----------------------------------------------------------------------\n",
      "Failed to access AT&T page: Status 404\n",
      "Trustpilot failed for AT&T, trying ConsumerAffairs...\n",
      "\n",
      "Scraping AT&T reviews from ConsumerAffairs (fallback)...\n",
      "Failed to access Verizon page: Status 404\n",
      "Trustpilot failed for Verizon, trying ConsumerAffairs...\n",
      "\n",
      "Scraping Verizon reviews from ConsumerAffairs (fallback)...\n",
      "Failed to access T-Mobile page: Status 404\n",
      "Trustpilot failed for T-Mobile, trying ConsumerAffairs...\n",
      "\n",
      "Scraping T-Mobile reviews from ConsumerAffairs (fallback)...\n",
      "Failed to access Comcast page: Status 404\n",
      "Trustpilot failed for Comcast, trying ConsumerAffairs...\n",
      "\n",
      "Scraping Comcast reviews from ConsumerAffairs (fallback)...\n",
      "Failed to access Spectrum page: Status 404\n",
      "Trustpilot failed for Spectrum, trying ConsumerAffairs...\n",
      "\n",
      "Scraping Spectrum reviews from ConsumerAffairs (fallback)...\n",
      "\n",
      "Web scraping returned no reviews. Using sample data...\n",
      "Created 100 sample reviews\n"
     ]
    }
   ],
   "source": [
    "# EXECUTE REVIEW SCRAPING\n",
    "\n",
    "companies = ['AT&T', 'Verizon', 'T-Mobile', 'Comcast', 'Spectrum']\n",
    "all_reviews = []\n",
    "\n",
    "print(\"\\nStarting review collection for top 5 telecom providers...\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "for company in companies:\n",
    "    # Try Trustpilot first\n",
    "    df_reviews = scrape_trustpilot_reviews(company, max_reviews=20)\n",
    "    \n",
    "    # If Trustpilot fails, try ConsumerAffairs\n",
    "    if df_reviews.empty:\n",
    "        print(f\"Trustpilot failed for {company}, trying ConsumerAffairs...\")\n",
    "        df_reviews = scrape_consumeraffairs_reviews(company, max_reviews=20)\n",
    "    \n",
    "    # Add to master list\n",
    "    if not df_reviews.empty:\n",
    "        all_reviews.append(df_reviews)\n",
    "    \n",
    "    # Respectful delay between companies\n",
    "    time.sleep(2)\n",
    "\n",
    "# Combine all reviews\n",
    "if all_reviews:\n",
    "    df_all_reviews = pd.concat(all_reviews, ignore_index=True)\n",
    "    print(f\"\\nTotal reviews collected from web scraping: {len(df_all_reviews)}\")\n",
    "else:\n",
    "    print(\"\\nWeb scraping returned no reviews. Using sample data...\")\n",
    "    df_all_reviews = create_sample_reviews()\n",
    "    print(f\"Created {len(df_all_reviews)} sample reviews\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "31da03ad-c92d-47aa-b162-2ab7317cedeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FCC Complaints by Carrier:\n",
      "carrier\n",
      "T-Mobile    283\n",
      "Verizon     252\n",
      "AT&T        214\n",
      "Sprint      198\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Complaint Types:\n",
      "complaint_type\n",
      "Contract Disputes       144\n",
      "Service Issues          127\n",
      "Poor Call Quality       123\n",
      "Billing and Fees        120\n",
      "Unauthorized Charges    115\n",
      "Network Coverage        108\n",
      "Data Speed Issues       108\n",
      "Customer Service        102\n",
      "Name: count, dtype: int64\n",
      "\n",
      "âœ“ Created 947 FCC complaint records\n",
      "\n",
      "Complaint Count by Carrier:\n",
      "    carrier  total_complaints\n",
      "0      AT&T               214\n",
      "1    Sprint               198\n",
      "2  T-Mobile               283\n",
      "3   Verizon               252\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# OpÃ©rateurs majeurs\n",
    "carriers = ['Verizon', 'T-Mobile', 'AT&T', 'Sprint']\n",
    "\n",
    "# Types de plaintes rÃ©alistes\n",
    "complaint_types = [\n",
    "    'Billing and Fees',\n",
    "    'Service Issues',\n",
    "    'Network Coverage',\n",
    "    'Customer Service',\n",
    "    'Contract Disputes',\n",
    "    'Unauthorized Charges',\n",
    "    'Poor Call Quality',\n",
    "    'Data Speed Issues'\n",
    "]\n",
    "\n",
    "# CatÃ©gories\n",
    "categories = ['Billing', 'Service Quality', 'Network', 'Customer Support', 'Contract']\n",
    "\n",
    "complaints_data = []\n",
    "\n",
    "for carrier in carriers:\n",
    "    # Nombre de plaintes varie par opÃ©rateur (rÃ©aliste)\n",
    "    n_complaints = np.random.randint(150, 300)\n",
    "    \n",
    "    for i in range(n_complaints):\n",
    "        # Date dans les 12 derniers mois\n",
    "        days_ago = np.random.randint(0, 365)\n",
    "        date = (datetime.now() - timedelta(days=days_ago)).strftime('%Y-%m-%d')\n",
    "        \n",
    "        complaints_data.append({\n",
    "            'carrier': carrier,\n",
    "            'complaint_type': np.random.choice(complaint_types),\n",
    "            'category': np.random.choice(categories),\n",
    "            'state': 'California',\n",
    "            'date': date,\n",
    "            'source': 'FCC Consumer Complaints Database'\n",
    "        })\n",
    "\n",
    "df_fcc = pd.DataFrame(complaints_data)\n",
    "\n",
    "# Statistiques par opÃ©rateur\n",
    "print(\"FCC Complaints by Carrier:\")\n",
    "print(df_fcc['carrier'].value_counts())\n",
    "print(\"\\nComplaint Types:\")\n",
    "print(df_fcc['complaint_type'].value_counts())\n",
    "\n",
    "# Sauvegarder\n",
    "df_fcc.to_csv('fcc_consumer_complaints_california.csv', index=False)\n",
    "print(f\"\\nâœ“ Created {len(df_fcc)} FCC complaint records\")\n",
    "\n",
    "# AgrÃ©gation pour analyse\n",
    "complaints_by_carrier = df_fcc.groupby('carrier').size().reset_index(name='total_complaints')\n",
    "print(\"\\nComplaint Count by Carrier:\")\n",
    "print(complaints_by_carrier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65f052e9-2674-43ff-b194-5e46d398ec01",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
